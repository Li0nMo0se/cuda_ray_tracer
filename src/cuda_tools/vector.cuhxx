#include "vector.cuh"

namespace cuda_tools
{

/* Kernels used internally by the vector to handle device memory */
namespace
{
template <typename T>
__global__ static void _kernel_dealloc(T** const data, const int32_t size)
{
    for (int32_t i = 0; i < size; i++)
        delete data[i];
}

template <typename T, typename SubT, typename... Ts>
__global__ static void _kernel_add_object(T** const data, Ts... args)
{
    // Data is already at the location in which the new element is stored
    *data = new SubT{args...};
}
} // namespace

template <typename T>
Vector<T>::Vector(const cudaStream_t stream)
    : Vector(begin_capacity, stream)
{
}

template <typename T>
Vector<T>::Vector(int32_t capacity, const cudaStream_t stream)
    : capacity_(capacity)
    , stream_(stream)
{
    realloc(capacity);
}

template <typename T>
Vector<T>::~Vector()
{
    if (data_)
    {
        // Implicit synchronization with the stream_
        _kernel_dealloc<<<1, 1, 0, stream_>>>(data_, size_);
        cuda_safe_call(cudaStreamSynchronize(stream_));
        check_error();
        cuda_safe_call(cudaFree(data_));
    }
}

template <typename T>
Vector<T>::Vector(Vector& other)
    : Vector(std::move(other))
{
}

template <typename T>
Vector<T>::Vector(Vector&& other)
{
    cuda_safe_call(cudaStreamSynchronize(other.stream_));

    // Copy fields
    size_ = other.size_;
    capacity_ = other.capacity_;
    data_ = other.data_;
    stream_ = other.stream_;

    // Empty other
    other.size_ = 0;
    other.capacity_ = 0;
    other.data_ = nullptr;
}

template <typename T>
void Vector<T>::realloc(const int32_t new_capacity)
{
    if (!data_)
        cuda_safe_call(cudaMalloc((void**)&data_, sizeof(T*) * new_capacity));
    else
    {
        T** tmp;
        cuda_safe_call(cudaMalloc((void**)&tmp, sizeof(T*) * new_capacity));
        cuda_safe_call(cudaMemcpyAsync(tmp,
                                       data_,
                                       sizeof(T*) * size_,
                                       cudaMemcpyDeviceToDevice,
                                       stream_));
        cuda_safe_call(cudaStreamSynchronize(stream_));
        cuda_safe_call(cudaFree(data_));
        data_ = tmp;
    }
    capacity_ = new_capacity;
}

template <typename T>
template <typename SubT, typename... Ts>
void Vector<T>::emplace_back(Ts&&... args)
{
    _kernel_add_object<T, SubT>
        <<<1, 1, 0, stream_>>>(&(data_[size_]), std::forward<Ts>(args)...);
    ++size_;

    // Upgrade the capacity
    if (size_ == capacity_)
        realloc(capacity_ * 2);
}

template <typename T>
__device__ inline int32_t Vector<T>::size_get() const
{
    return size_;
}

template <typename T>
__device__ inline const T& Vector<T>::operator[](const int32_t pos) const
{
    assert(pos < size_);
    return *(data_[pos]);
}

template <typename T>
const T** Vector<T>::back_get() const
{
    // Wait for every kernels and copying to finish
    cuda_safe_call(cudaStreamSynchronize(stream_));

    if (size_ == 0)
        return nullptr;
    T** res = new T*;
    cuda_safe_call(cudaMemcpyAsync(res,
                                   data_ + size_ - 1,
                                   sizeof(T*),
                                   cudaMemcpyDeviceToHost,
                                   stream_));

    // Return when the copy is finished
    cuda_safe_call(cudaStreamSynchronize(stream_));
    return const_cast<const T**>(res);
}

} // namespace cuda_tools